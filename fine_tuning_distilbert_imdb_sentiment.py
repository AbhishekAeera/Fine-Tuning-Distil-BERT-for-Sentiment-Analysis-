# -*- coding: utf-8 -*-
"""fine_tuning_distilbert_imdb_sentiment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_MCIRt9KK8bPSHfDOu83qEA25axT2LIJ
"""

!pip install transformers datasets scikit-learn

!pip install -U transformers datasets scikit-learn huggingface_hub

from datasets import load_dataset
from transformers import (
    DistilBertTokenizerFast,
    DistilBertForSequenceClassification,
    TrainingArguments,
    Trainer,
    pipeline,
    AutoModelForSequenceClassification,
    AutoTokenizer
)
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from huggingface_hub import notebook_login
import numpy as np
import os
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import torch

os.environ["WANDB_DISABLED"] = "true"

dataset = load_dataset("imdb")

# Print basic info
print(dataset)
print(dataset["train"][0])

tokenizer = DistilBertTokenizerFast.from_pretrained("distilbert-base-uncased")

def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        padding="max_length",
        max_length=256
    )

tokenized_datasets = dataset.map(tokenize_function, batched=True)
tokenized_datasets = tokenized_datasets.remove_columns(["text"])
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
tokenized_datasets.set_format("torch")

train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(500))
test_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(200))

training_args = TrainingArguments(
    output_dir="./results",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=16,
    num_train_epochs=1,
    logging_dir="./logs",
    logging_steps=10,
    report_to="none",
    fp16=True  # Mixed precision = faster on GPUs that support it
)

def compute_metrics(pred):
    labels = pred.label_ids
    preds = np.argmax(pred.predictions, axis=1)
    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, preds, average='macro', zero_division=0
    )
    acc = accuracy_score(labels, preds)
    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

model = DistilBertForSequenceClassification.from_pretrained(
    "distilbert-base-uncased",
    num_labels=2
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

trainer.train()

model.save_pretrained("./distilbert-imdb-sentiment")
tokenizer.save_pretrained("./distilbert-imdb-sentiment")

results = trainer.evaluate()
print(results)

sentiment_pipe = pipeline(
    "text-classification",
    model="./distilbert-imdb-sentiment",
    tokenizer="./distilbert-imdb-sentiment"
)

print(sentiment_pipe("This movie was fantastic and kept me entertained!"))
print(sentiment_pipe("The film was boring and poorly acted."))

label_map = {
    "LABEL_0": "negative",
    "LABEL_1": "positive"
}

texts = [
    "Absolutely loved this film!",
    "Worst movie ever. Don't watch it.",
    "It was okay. Not the best, not the worst.",
    "This movie exceeded my expectations with great acting and story!",
    "I found the plot boring and predictable."
]

for text in texts:
    result = sentiment_pipe(text)[0]
    sentiment = label_map[result["label"]]
    score = result["score"]
    print(f"Text: {text}")
    print(f"Predicted sentiment: {sentiment} (confidence: {score:.2f})\n")

from huggingface_hub import notebook_login

notebook_login()

model_id = "abhishekaeera/imdb-sentiment-model"

# Reload from saved path
model = AutoModelForSequenceClassification.from_pretrained("./distilbert-imdb-sentiment")
tokenizer = AutoTokenizer.from_pretrained("./distilbert-imdb-sentiment")

# Upload
model.push_to_hub(model_id)
tokenizer.push_to_hub(model_id)

sentiment_pipe = pipeline(
    "text-classification",
    model="abhishekaeera/imdb-sentiment-model"
)

texts = [
    "This movie was absolutely brilliant and fun!",
    "I hated this film so much.",
    "It was okay, not great but not terrible either."
]

label_map = {
    "LABEL_0": "negative",
    "LABEL_1": "positive"
}

for text in texts:
    result = sentiment_pipe(text)[0]
    sentiment = label_map[result["label"]]
    score = result["score"]
    print(f"Text: {text}")
    print(f"Predicted sentiment: {sentiment} (confidence: {score:.2f})\n")

# Get predictions on your test dataset
# (Uses small test subset for speed)

# Put model in eval mode
model.eval()

all_labels = []
all_preds = []

# Disable gradient computation for faster inference
with torch.no_grad():
    for batch in torch.utils.data.DataLoader(test_dataset, batch_size=16):
        input_ids = batch["input_ids"].to(model.device)
        attention_mask = batch["attention_mask"].to(model.device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        predictions = torch.argmax(logits, dim=-1)

        all_preds.extend(predictions.cpu().numpy())
        all_labels.extend(batch["labels"].cpu().numpy())

# Compute confusion matrix
cm = confusion_matrix(all_labels, all_preds, labels=[0, 1])

# Plot confusion matrix
disp = ConfusionMatrixDisplay(
    confusion_matrix=cm,
    display_labels=["negative", "positive"]
)
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix - IMDB Sentiment Analysis")
plt.show()